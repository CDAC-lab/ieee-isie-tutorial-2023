{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CDAC-lab/isie2023/blob/main/tutorial-notebook-3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exposing an AI based Question Answering Assistant on top of media content\n",
        "\n",
        "# Overview\n",
        "\n",
        "This notebook is designed to demonstrate an end-to-end pipeline leveraging the capabilities of OpenAI's Whisper, ChromaDB, and Langchain to enable intelligent querying of YouTube videos. We begin by taking a YouTube video URL, from which the audio is extracted and transcribed using Whisper, OpenAI's automatic speech recognition (ASR) system. This transcription is then vectorized using ChromaDB, a high-performance vector database, effectively transforming the unstructured text data into a structured, queryable form. Finally, Langchain is utilized to provide a natural language interface for querying the stored vector data, allowing users to extract meaningful information from the video content.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction and Setting Up](#section1)\n",
        "    - Introduction to the Notebook\n",
        "    - Installing Necessary Libraries\n",
        "    - Importing Libraries and Dependencies\n",
        "2. [Data Acquisition](#section2)\n",
        "    - Getting Video Data from YouTube\n",
        "    - Extracting Audio from YouTube Video\n",
        "3. [Transcription using Whisper](#section3)\n",
        "    - Introduction to Whisper\n",
        "    - Transcribing Audio to Text\n",
        "4. [Vectorization using ChromaDB](#section4)\n",
        "    - Introduction to ChromaDB\n",
        "    - Preprocessing Text for Vectorization\n",
        "    - Vectorizing Text Data\n",
        "5. [Querying with Langchain](#section5)\n",
        "    - Introduction to Langchain\n",
        "    - Setting Up Langchain for Querying\n",
        "    - Formulating and Executing Queries\n",
        "6. [Analysis and Visualization](#section6)\n",
        "    - Analyzing Query Results\n",
        "    - Visualizing Query Results\n",
        "7. [Conclusion and Possible Extensions](#section7)\n",
        "    - Summary of Achievements\n",
        "    - Potential Future Work\n",
        "8. [References and Additional Resources](#section8)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "92pgzYPbqiRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction and Setting Up\n",
        "\n",
        "## Introduction to the Notebook\n",
        "Welcome to our notebook! This project aims to create an end-to-end pipeline to extract, process, vectorize, and query the content of YouTube videos. By using state-of-the-art tools like Whisper, ChromaDB, and Langchain, we aim to transform unstructured video content into a structured and easily searchable form.\n",
        "\n",
        "## Installing Necessary Libraries\n",
        "In this section, we'll guide you through the installation process for all the necessary libraries that we'll use throughout this notebook. This includes OpenAI's Whisper for speech recognition, ChromaDB for vectorization, and Langchain for natural language querying.\n",
        "\n",
        "## Importing Libraries and Dependencies\n",
        "Here, we will import all the required Python libraries and dependencies that we'll be using in our notebook. This includes standard libraries for data handling and manipulation, as well as libraries specific to our pipeline such as the API wrappers for Whisper, ChromaDB, and Langchain."
      ],
      "metadata": {
        "id": "3_ddlZvzzN5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install libraries"
      ],
      "metadata": {
        "id": "H_juZ2X-OP1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qqq install git+https://github.com/openai/whisper.git\n",
        "!pip -qqq install pytube\n",
        "!pip install langchain\n",
        "!pip install chromadb\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "7N1JGqD-KGgV",
        "outputId": "3ec8cde4-e65f-4d60-f27a-77c7b2a89c69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.204-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.9 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.10-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.8 frozenlist-1.3.3 langchain-0.0.204 langchainplus-sdk-0.0.10 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.3.26-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Collecting requests>=2.28 (from chromadb)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.7)\n",
            "Collecting hnswlib>=0.7 (from chromadb)\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb)\n",
            "  Downloading clickhouse_connect-0.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.1/965.1 kB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.1)\n",
            "Collecting fastapi>=0.85.1 (from chromadb)\n",
            "  Downloading fastapi-0.97.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.65.0)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.7.1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb)\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb)\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.85.1->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.6.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2119666 sha256=3b1067c79d92f104f1025fc1b01de26c527fb959fd944a92b65ef09fd01986b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: tokenizers, monotonic, zstandard, websockets, uvloop, requests, python-dotenv, pulsar-client, overrides, lz4, humanfriendly, httptools, hnswlib, h11, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, clickhouse-connect, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chromadb-0.3.26 clickhouse-connect-0.6.3 coloredlogs-15.0.1 fastapi-0.97.0 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 humanfriendly-10.0 lz4-4.3.2 monotonic-1.6 onnxruntime-1.15.1 overrides-7.3.1 posthog-3.0.1 pulsar-client-3.2.0 python-dotenv-1.0.0 requests-2.31.0 starlette-0.27.0 tokenizers-0.13.3 uvicorn-0.22.0 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 zstandard-0.21.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "I-BlQeoaKGvG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyBfYgKpNzuE"
      },
      "outputs": [],
      "source": [
        "#libraries for google drive authentication\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import whisper\n",
        "import torch\n",
        "import os\n",
        "from pytube import YouTube\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import DataFrameLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "\n",
        "# Set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the model\n",
        "whisper_model = whisper.load_model(\"large\", device=device)"
      ],
      "metadata": {
        "id": "yQmc9_K4OXPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a552ebc-0458-4720-9c1f-b281614d02ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.87G/2.87G [00:12<00:00, 246MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Acquisition\n",
        "\n",
        "## Getting Video Data from YouTube\n",
        "In this section, we'll discuss how to input a YouTube video URL and use it to extract the video data. This involves using a YouTube data extraction library to access and download the video.\n",
        "\n",
        "## Extracting Audio from YouTube Video\n",
        "After obtaining the video, the next step is to extract the audio which will be transcribed into text. We'll discuss the method used to perform this extraction and the format in which the audio data is saved.\n",
        "\n",
        "# Transcription using Whisper\n",
        "\n",
        "## Introduction to Whisper\n",
        "Whisper is OpenAI's automatic speech recognition (ASR) system. In this section, we'll provide a brief introduction to Whisper and explain how it is used to transcribe the audio from our YouTube video.\n",
        "\n",
        "## Transcribing Audio to Text\n",
        "Here, we'll walk you through the process of transcribing the extracted audio into text using Whisper. This involves sending the audio data to the Whisper API and receiving a text transcript in return."
      ],
      "metadata": {
        "id": "qfXYH59i3oMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract the audio from youtube video"
      ],
      "metadata": {
        "id": "EuqnkSwXLHNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_save_audio(video_URL, destination, final_filename):\n",
        "  video = YouTube(video_URL)#get video\n",
        "  audio = video.streams.filter(only_audio=True).first()#seperate audio\n",
        "  output = audio.download(output_path = destination)#download and save for transcription\n",
        "  _, ext = os.path.splitext(output)\n",
        "  new_file = final_filename + '.mp3'\n",
        "  os.rename(output, new_file)"
      ],
      "metadata": {
        "id": "PNN0oi6wiW0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Video to audio\n",
        "video_URL = 'https://www.youtube.com/watch?v=3G5hWM6jqPk'\n",
        "destination = \".\"\n",
        "final_filename = \"MIT 6.S191: Deep Generative Modeling\"\n",
        "extract_and_save_audio(video_URL, destination, final_filename)"
      ],
      "metadata": {
        "id": "QSwh2yMrLZV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribe"
      ],
      "metadata": {
        "id": "716kqAfWLS8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run the whisper model\n",
        "audio_file = \"MIT 6.S191: Deep Generative Modeling.mp3\"\n",
        "result = whisper_model.transcribe(audio_file)"
      ],
      "metadata": {
        "id": "avu00PEtdJjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a 1 hour lecture, it takes around 10 minutes to complete the transcription. For the time being, a pre-transcribed version is being loaded from the disk."
      ],
      "metadata": {
        "id": "7FkY8A1AdJqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#authenticate with you google drive credentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# This is the file ID of the data set, this will download the datafile from the shared location\n",
        "transcription_id = '14Cqyn3ND_X8PUNkIujYr0qUpJ6xdj8d6'\n",
        "transcription_data = drive.CreateFile({'id':transcription_id})\n",
        "transcription_data.GetContentFile('transcription.csv')"
      ],
      "metadata": {
        "id": "2yjFdMojcth7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunk Clips"
      ],
      "metadata": {
        "id": "c7UsxaG9XpXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = pd.read_csv('transcription.csv')\n",
        "\n",
        "\n",
        "def chunk_clips(transcription, clip_size):\n",
        "  texts = []\n",
        "  sources = []\n",
        "  for i in range(0,len(transcription),clip_size):\n",
        "    clip_df = transcription.iloc[i:i+clip_size,:]\n",
        "    text = \" \".join(clip_df['text'].to_list())\n",
        "    source = str(round(clip_df.iloc[0]['start']/60,2))+ \" - \"+str(round(clip_df.iloc[-1]['end']/60,2)) + \" min\"\n",
        "    print(text)\n",
        "    print(source)\n",
        "    texts.append(text)\n",
        "    sources.append(source)\n",
        "\n",
        "  return [texts,sources]\n",
        "\n"
      ],
      "metadata": {
        "id": "03XK_EkvNFRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunk_clips(transcription, 50)\n",
        "documents = chunks[0]\n",
        "sources = chunks[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH7FhnoeYIlI",
        "outputId": "d68aa52b-2ea4-4d3b-d65b-708a5f920017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I'm really, really excited about this lecture because as Alexander introduced  yesterday, right now we're in this tremendous age of generative AI. And  today we're going to learn the foundations of deep generative modeling,  where we're going to talk about building systems that can not only look for  patterns in data, but can actually go a step beyond this to generate brand new  data instances based on those learned patterns. This is an incredibly complex  and powerful idea, and as I mentioned it's a particular subset of deep  learning that has actually really exploded in the past couple of years and  this year in particular. So to start and to demonstrate how powerful these  algorithms are, let me show you these three different faces. I want you to  take a minute, think. Think about which face you think is real. Raise your hand  if you think it's face A. Okay, I see a couple of people. Face B. Many more people.  Face C. About second place. Well the truth is that all of you are wrong. All three of  these faces are fake. These people do not exist. These images were synthesized by  deep generative models trained on data of human faces and asked to produce new  instances. Now I think that this demonstration kind of demonstrates the  power of these ideas and the power of this notion of generative modeling. So  let's get a little more concrete about how we can formalize this. So far in this  course, we've been looking at what we call problems of supervised learning,  meaning that we're given data and associated with that data is a set of  labels. Our goal is to learn a function that maps that data to the labels. Now  we're in a course on deep learning, so we've been concerned with functional  mappings that are defined by deep neural networks, but really that function could  be anything. Neural networks are powerful, but we could use other techniques as  well. In contrast, there's another class of problems in machine learning that we  refer to as unsupervised learning, where we take data, but now we're given only  data, no labels, and our goal is to try to build some method that can understand  the hidden underlying structure of that data. What this allows us to do is it  gives us new insights into the foundational representation of the data  and, as we'll see later, actually enables us to generate new data instances. Now  this class of problems, this definition of unsupervised learning, captures the  types of models that we're going to talk about today in the focus on  generative modeling, which is an example of unsupervised learning and is united  by this goal of the problem where we're given only samples from a training set  and we want to learn a model that represents the distribution of the data  that the model is seeing. Generative modeling takes two general forms. First,  density estimation and second, sample generation. In density estimation, the  task is, given some data examples, our goal is to train a model that learns a  underlying probability distribution that describes where the data came from.  With sample generation, the idea is similar, but the focus is more on  actually generating new instances. Our goal with sample generation is to, again,  learn this model of this underlying probability distribution, but then use  that model to sample from it and generate new instances that are similar  to the data that we've seen, approximately following along, ideally,  that same real data distribution. Now in both these cases of density estimation  and sample generation, the underlying question is the same. Our learning task  is to try to build a model that learns this probability distribution that is as  close as possible to the true data distribution. Okay, so with this  definition and this concept of generative modeling, what are some ways  that we can actually deploy generative modeling forward in the real world for\n",
            "0.0 - 4.95 min\n",
            " high-impact applications? Well, part of the reason that generative models are  so powerful is that they have this ability to uncover the underlying  features in a data set and encode it in an efficient way. So for example, if we're  considering the problem of facial detection and we're given a data set  with many, many different faces, starting out without inspecting this data, we may  not know what the distribution of faces in this data set is with respect to  features we may be caring about. For example, the pose of the head, clothing,  glasses, skin tone, hair, etc. And it can be the case that our training data may be  very, very biased towards particular features without us even realizing this.  Using generative models, we can actually identify the distributions of these  underlying features in a completely automatic way without any labeling in  order to understand what features may be over-represented in the data, what  features may be under-represented in the data. And this is the focus of today and  tomorrow's software labs, which are going to be part of the software lab  competition. Developing generative models that can do this task and using it to  uncover and diagnose biases that can exist within facial detection models.  Another really powerful example is in the case of outlier detection, identifying  rare events. So let's consider the example of self-driving autonomous cars.  With an autonomous car, let's say it's driving out in the real world, we really,  really want to make sure that that car can be able to handle all the possible  scenarios and all the possible cases it may encounter, including edge cases like  a deer coming in front of the car or some unexpected rare events. Not just, you  know, the typical straight freeway driving that it may see the majority of  the time. With generative models, we can use this idea of density estimation to  be able to identify rare and anomalous events within the training data and as  they're occurring as the model sees them for the first time. So hopefully  this paints a picture of what generative modeling, the underlying concept, is and a  couple of different ways in which we can actually deploy these ideas for powerful  and impactful real-world applications. In today's lecture, we're going to focus on  a broad class of generative models that we call latent variable models and  specifically distilled down into two subtypes of latent variable models. First  things first, I've introduced this term latent variable but I haven't told you  or described to you what that actually is. I think a great example and one of my  favorite examples throughout this entire course that gets at this idea of the  latent variable is this little story from Plato's Republic, which is known as  the myth of the cave. In this myth, there is a group of prisoners and as part of  their punishment, they're constrained to face a wall. Now the only things that  prisoners can observe are shadows of objects that are passing in front of a  fire that's behind them and they're observing the casting of the shadows on  the wall of this cave. To the prisoners, those shadows are the only things they  see, their observations. They can measure them, they can give them names because to  them, that's their reality. But they're unable to directly see the underlying  objects, the true factors themselves that are casting those shadows. Those objects  here are like latent variables in machine learning. They're not directly  observable but they're the true underlying features or explanatory  factors that create the observed differences and variables that we can  see and observe. And this gets at the goal of generative modeling, which is to  find ways that we can actually learn these hidden features, these underlying  latent variables, even when we're only given observations of the observed data.  So let's start by discussing a very simple generative model that tries to do\n",
            "4.95 - 9.6 min\n",
            " this through the idea of encoding the data input. The models we're going to  talk about are called autoencoders and to take a look at how an autoencoder  works, we'll go through step by step, starting with the first step of taking  some raw input data and passing it through a series of neural network  layers. Now the output of this first step is what we refer to as a  low-dimensional latent space. It's an encoded representation of those  underlying features and that's our goal in trying to train this model and  predict those features. The reason a model like this is called an encoder, an  autoencoder, is that it's mapping the data, X, into this vector of latent  variables, Z. Now let's ask ourselves a question, let's pause for a moment. Why  may we care about having this latent variable vector Z be in a low-dimensional  space? Anyone have any ideas?  All right, maybe there are some ideas. Yes?  The suggestion was that it's more efficient. Yes, that gets at the  heart of the question. The idea of having that low-dimensional latent space  is that it's a very efficient, compact encoding of the rich, high-dimensional  data that we may start with. As you pointed out, right, what this means is  that we're able to compress data into this small feature representation, a  vector, that captures this compactness and richness without requiring so much  memory or so much storage. So how do we actually train the network to learn this  latent variable vector? Since we don't have training data, we can't explicitly  observe these latent variables Z, we need to do something more clever. What the  autoencoder does is it builds a way to decode this latent variable vector back  up to the original data space, trying to reconstruct the original image from that  compressed, efficient latent encoding. And once again, we can use a series of  neural network layers, such as convolutional layers, fully connected  layers, but now to map back from that lower dimensional space back upwards to  the input space. This generates a reconstructed output, which we can denote  as x-hat, since it's an imperfect reconstruction of our original input  data. To train this network, all we have to do is compare the outputted  reconstruction and the original input data and say, how do we make these as  similar as possible? We can minimize the distance between that input and our  reconstructed output. So for example, for an image, we can compare the pixel-wise  difference between the input data and the reconstructed output, just  subtracting the images from one another and squaring that difference to capture  the pixel-wise divergence between the input and the reconstruction. What I hope  you'll notice and appreciate is in that definition of the loss, it doesn't  require any labels. The only components of that loss are the original input data  x and the reconstructed output x-hat. So I've simplified now this diagram by  abstracting away those individual neural network layers in both the encoder and  decoder components of this. And again, this idea of not requiring any labels  gets back to the idea of unsupervised learning, since what we've done is we've  been able to learn a encoded quantity, our latent variables, that we cannot  observe without any explicit labels. All we started from was the raw data itself.  It turns out that as the question and answer got at, that dimensionality of  the latent space has a huge impact on the quality of the generated  reconstructions and how compressed that information bottleneck is.  Auto-encoding is a form of compression, and so the lower the dimensionality of  the latent space, the less good our reconstructions are going to be, but the  higher the dimensionality, the more, the less efficient that encoding is going to be.\n",
            "9.6 - 14.45 min\n",
            " So to summarize this first part, this idea of an autoencoder is using  this bottlenecked, compressed, hidden latent layer to try to bring the network  down to learn a compact, efficient representation of the data. We don't  require any labels, this is completely unsupervised, and so in this way we're  able to automatically encode information within the data itself to learn this  latent space, auto-encoding information, auto-encoding data.  Now this is a pretty simple model, and it turns out that in practice this idea  of self-encoding or auto-encoding has a bit of a twist on it to allow us to  actually generate new examples that are not only reconstructions of the input  data itself, and this leads us to the concept of variational auto-encoders or  VAEs. With the traditional auto-encoder that we just saw, if we pay closer  attention to the latent layer, right, which is shown in that orange salmon  color, that latent layer is just a normal layer in the neural network. It's  completely deterministic. What that means is once we've trained the network, once  the weights are set, any time we pass a given input in and go back through the  latent layer, decode back out, we're going to get the same exact reconstruction. The  weights aren't changing, it's deterministic. In contrast, variational  auto-encoders, VAEs, introduce a element of randomness, a probabilistic twist on  this idea of auto-encoding. What this will allow us to do is to actually  generate new images similar to the, or new data instances that are similar to  the input data, but not forced to be strict reconstructions. In practice, with  the variational auto-encoder, we've replaced that single deterministic layer  with a random sampling operation. Now, instead of learning just the latent  variables directly themselves, for each latent variable we define a mean and a  standard deviation that captures a probability distribution over that  latent variable. What we've done is we've gone from a single vector of latent  variable Z to a vector of means mu and a vector of standard deviations sigma that  parametrized the probability distributions around those latent  variables. What this will allow us to do is now sample, using this element of  randomness, this element of probability, to then obtain a probabilistic  representation of the latent space itself. As you hopefully can tell, right,  this is very, very, very similar to the auto-encoder itself, but we've just added  this probabilistic twist where we can sample in that intermediate space to get  these samples of latent variables. Okay, now to get a little more into the depth  of how this is actually learned, how this is actually trained, with defining the  VAE, we've eliminated this deterministic nature to now have these encoders and  decoders that are probabilistic. The encoder is computing a probability  distribution of the latent variable Z given input data X, while the decoder is  doing the inverse, trying to learn a probability distribution back in the  input data space given the latent variables Z. And we define separate sets  of weights, phi and theta, to define the network weights for the encoder and  decoder components of the VAE. Alright, so when we get now to how we actually  optimize and learn the network weights in VAE, first step is to define a loss  function, right? That's the core element to training a neural network. Our loss is  going to be a function of the data and a function of the neural network weights,  just like before, but we have these two components, these two terms that define  our VAE loss. First, we see the reconstruction loss, just like before,  where the goal is to capture the difference between our input data and  the reconstructed output, and now for the VAE, we've introduced a second term to  the loss, what we call the regularization term. Often you'll maybe\n",
            "14.45 - 19.2 min\n",
            " even see this referred to as a VAE loss, and we'll go into  describing what this regularization term means and what it's doing.  To do that and to understand, remember and keep in mind that in all neural  network operations, our goal is to try to optimize network weights with respect  to the data, with respect to minimizing this objective loss, and so here we're  concerned with the network weights phi and theta that define the weights of the  encoder and the decoder. We consider these two terms, first the reconstruction  loss. Again, the reconstruction loss is very, very similar, same as before. You can  think of it as the error or the likelihood that effectively captures  the difference between your input and your outputs, and again we can trade this  in an unsupervised way, not requiring any labels to force the latent space and  the network to learn how to effectively reconstruct the input data. The second  term, the regularization term, is now where things get a bit more interesting,  so let's go on into this in a little bit more detail. Because we have this  probability distribution and we're trying to compute this encoding and then  decode back up. As part of regularizing, we want to take that  inference over the latent distribution and constrain it to behave nicely, if you  will. The way we do that is we place what we call a prior on the latent  distribution, and what this is is some initial hypothesis or guess about what  that latent variable space may look like. This helps us and helps the network  to enforce a latent space that roughly tries to follow this prior distribution,  and this prior is denoted as P of Z, right? That term D, that's effectively the  regularization term. It's capturing a distance between our encoding of the  latent variables and our prior hypothesis about what the structure of  that latent space should look like. So over the course of training, we're trying  to enforce that each of those latent variables adopts a  probability distribution that's similar to that prior. A common choice when  training VAEs and developing these models is to enforce the latent  variables to be roughly standard, normal, Gaussian distributions, meaning that  they are centered around mean zero and they have a standard deviation of one.  What this allows us to do is to encourage the encoder to put the latent  variables roughly around a centered space, distributing the encoding smoothly  so that we don't get too much divergence away from that smooth space, which can  occur if the network tries to cheat and try to simply memorize the data. By  placing the Gaussian standard normal prior on the latent space, we can define  a concrete mathematical term that captures the distance, the divergence  between our encoded latent variables and this prior, and this is called the KL  divergence. When our prior is a standard normal, the KL divergence takes the form  of the equation that I'm showing up on the screen, but what I want you to really  get away, come away with is that the concept of trying to smooth things out  and to capture this divergence and this difference between the prior and the  latent encoding is all this KL term is trying to capture. So it's a bit of math  and I acknowledge that, but what I want to next go into is really what is the  intuition behind this regularization operation. Why do we do this and why does  the normal prior in particular work effectively for VAEs? So let's consider  what properties we want our latent space to adopt and for this regularization to  achieve. The first is this goal of continuity. We don't, and what we mean by  continuity is that if there are points in the latent space that are close  together, ideally after decoding we should recover two reconstructions that  are similar in content that make sense that they're close together. The second\n",
            "19.2 - 24.07 min\n",
            " key property is this idea of completeness. We don't want there to be  gaps in the latent space. We want to be able to decode and sample from the  latent space in a way that is smooth and a way that is connected. To get more  concrete, what, let's ask what could be the consequences of not regularizing our  latent space at all? Well, if we don't regularize, we can end up with instances  where there are points that are close in the latent space but don't end up with  similar decodings or similar reconstructions. Similarly, we could have  points that don't lead to meaningful reconstructions at all. They're somehow  encoded but we can't decode effectively. Regularization allows us to realize  points that end up close in the latent space and also are similarly  reconstructed and meaningfully reconstructed. Okay, so continuing with  this example, the example that I showed there and I didn't get into details was  showing these shapes, these shapes of different colors and that we're trying  to be encoded in some lower dimensional space. With regularization, we are able to  achieve this by trying to minimize that regularization term. It's not sufficient  to just employ the reconstruction loss alone to achieve this continuity and  this completeness because of the fact that without regularization, just  encoding and reconstructing does not guarantee the properties of continuity  and completeness. We overcome this, these issues of having potentially pointed  distributions, having discontinuities, having disparate means that could end up  in the latent space without the effect of regularization. We overcome this by  now regularizing the mean and the variance of the encoded latent  distributions according to that normal prior. What this allows is for the  learned distributions of those latent variables to effectively overlap in the  latent space because everything is regularized to have, according to this  prior of mean zero, standard deviation one and that centers the means, regularizes  the variances for each of those independent latent variable  distributions. Together, the effect of this regularization in net is that we  can achieve continuity and completeness in the latent space. Points and distances  that are close should correspond to similar reconstructions that we get out.  So hopefully this gets at some of the intuition behind the idea of the  VAE, behind the idea of the regularization and trying to enforce the  structured normal prior on the latent space. With this in hand, with the two  components of our loss function, reconstructing the inputs, regularizing  learning to try to achieve continuity and completeness, we can now think about  how we define a forward pass through the network, going from an input example and  being able to decode and sample from the latent variables to look at new examples.  Our last critical step is how the actual backpropagation training algorithm is  defined and how we achieve this. The key, as I introduced with VAEs, is this notion  of randomness of sampling that we have introduced by defining these probability  distributions over each of the latent variables. The problem this gives us is  that we cannot back propagate directly through anything that has an element of  sampling, anything that has an element of randomness. Backpropagation requires  completely deterministic nodes, deterministic layers to be able to  successfully apply gradient descent and the backpropagation algorithm. The  breakthrough idea that enabled VAEs to be trained completely end-to-end was  this idea of reparameterization within that sampling layer, and I'll give you  the key idea about how this operation works. It's actually really quite  clever. So, as I said, when we have a notion of randomness of probability, we  can't sample directly through that layer. Instead, with reparameterization,\n",
            "24.07 - 28.8 min\n",
            " what we do is we redefine how a latent variable vector is sampled as a sum of a  fixed deterministic mean mu, a fixed vector of standard deviation sigma, and  now the trick is that we divert all the randomness, all the sampling, to a random  constant epsilon that's drawn from a normal distribution. So, mean itself is  fixed, standard deviation is fixed, all the randomness and the sampling occurs  according to that epsilon constant. We can then scale the mean and standard  deviation by that random constant to re-achieve the sampling operation within  the latent variables themselves. What this actually looks like, and an  illustration that breaks down this concept of reparameterization and  divergence is as follows. So, looking here, right, what I've shown is  these completely deterministic steps in blue, and the sampling random steps in  orange. Originally, if our latent variables are what effectively are  capturing the randomness, the sampling themselves, we have this problem in that  we can't back propagate, we can't train directly through anything that has  stochasticity, that has randomness. What reparameterization allows us to do is it  shifts this diagram, where now we've completely diverted that sampling  operation off to the side to this constant epsilon, which is drawn from a  normal prior, and now when we look back at our latent variable, it is  deterministic with respect to that sampling operation. What this means is  that we can back propagate to update our network weights completely end-to-end  without having to worry about direct randomness, direct stochasticity within  those latent variables Z. This trick is really, really powerful because it  enabled the ability to train these VAEs completely end-to-end in a, through a  back propagation algorithm. Alright, so at this point, we've gone through the core  architecture of VAEs, we've introduced these two terms of the loss, we've seen  how we can train it end-to-end. Now let's consider what these latent variables are  actually capturing and what they represent. When we impose this  distributional prior, what it allows us to do is to sample effectively from the  latent space and actually slowly perturb the value of single latent  variables, keeping the other ones fixed, and what you can observe and what you  can see here is that by doing that perturbation, that tuning of the value of  the latent variables, we can run the decoder of the VAE every time, reconstruct  the output every time we do that tuning, and what you'll see, hopefully with this  example with the face, is that an individual latent variable is capturing  something semantically informative, something meaningful, and we see that by  this perturbation, by this tuning. In this example, the face, as you hopefully can  appreciate, is shifting, the pose is shifting, and all this is driven by is  the perturbation of a single latent variable, tuning the value of that latent  variable and seeing how that affects the decoded reconstruction. The network  is actually able to learn these different encoded features, these  different latent variables, such that by perturbing the values of them  individually, we can interpret and make sense of what those latent variables  mean and what they represent. To make this more concrete, right, we can consider  even multiple latent variables simultaneously, compare one against the  other, and ideally we want those latent features to be as independent as  possible in order to get at the most compact and richest representation and  compact encoding. So here, again in this example of faces, we're walking along two  axes, head pose on the x-axis, and what appears to be kind of a notion of a  smile on the y-axis, and you can see that with these reconstructions, we can  actually perturb these features to be able to perturb the end effect in the\n",
            "28.8 - 33.52 min\n",
            " reconstructed space. And so ultimately, with a VAE, our goal is to try to  enforce as much information to be captured in that encoding as possible. We  want these latent features to be independent and ideally disentangled. It  turns out that there is a very clever and simple way to try to encourage this  independence and this disentanglement. While this may look a little complicated  with the math and a bit scary, I will break this down with the idea of  how a very simple concept enforces this independent latent encoding and this  disentanglement. All this term is showing is those two components of the loss, the  reconstruction term, the regularization term. That's what I want you to focus on.  The idea of latent space disentanglement really arose with this concept of beta  VAEs. What beta VAEs do is they introduce this parameter, beta, and what it is, it's  a weighting constant. The weighting constant controls how powerful that  regularization term is in the overall loss of the VAE, and it turns out that by  increasing the value of beta, you can try to encourage greater disentanglement,  more efficient encoding to enforce these latent variables to be uncorrelated  with each other. Now, if you're interested in mathematically why beta VAEs enforce  this disentanglement, there are many papers in the literature and proofs and  discussions as to why this occurs, and we can point you in those directions. But to  get a sense of what this actually affects downstream, when we look at face  reconstruction as a task of interest, with the standard VAE, no beta term or  rather a beta of one, you can hopefully appreciate that the features of the  rotation of the head, the pose and the rotation of the head, is also actually  ends up being correlated with smile and the facial, the mouth expression and the  mouth position, in that as the head pose is changing, the apparent smile or the  position of the mouth is also changing. But with beta VAEs, empirically we can  observe that with imposing these beta values much, much, much greater than one,  we can try to enforce greater disentanglement, where now we can  consider only a single latent variable, head pose, and the smile, the position of  the mouth in these images is more constant compared to the standard VAE.  Alright, so this is really all the core math, the core operations, the core  architecture of VAEs that we're going to cover in today's lecture and in this  class in general. To close this section and as a final note, I want to remind you  back to the motivating example that I introduced at the beginning of this  lecture, facial detection, where now hopefully you've understood this concept  of latent variable learning and encoding and how this may be useful for a task  like facial detection, where we may want to learn those distributions of the  underlying features in the data. And indeed, you're going to get hands-on  practice in the software labs to build variational autoencoders that can  automatically uncover features underlying facial detection data sets  and use this to actually understand underlying and hidden biases that may  exist with those data and with those models. And it doesn't just stop there.  Tomorrow we'll have a very, very exciting guest lecture on robust and trustworthy  deep learning, which will take this concept a step further to realize how we  can use this idea of generative models and latent variable learning to not only  uncover and diagnose biases, but actually solve and mitigate some of those harmful  effects of those biases in neural networks for facial detection and other  applications. Alright, so to summarize quickly the key points of VAEs, we've  gone through how they're able to compress data into this compact encoded  representation. From this representation we can generate reconstructions of the  input in a completely unsupervised fashion. We can train them end-to-end\n",
            "33.52 - 38.33 min\n",
            " using the reparameterization trick. We can understand the semantic  interpretation of individual latent variables by perturbing their values. And  finally, we can sample from the latent space to generate new examples by  passing back up through the decoder. So VAEs are looking at this idea of latent  variable encoding and density estimation as their core problem. What if now we  only focus on the quality of the generated samples and that's the task  that we care more about? For that we're going to transition to a new type of  generative model called a generative adversarial network or GAN, where with  GANs our goal is really that we care more about how well we generate new  instances that are similar to the existing data, meaning that we want to  try to sample from a potentially very complex distribution that the model is  trying to approximate. It can be extremely, extremely difficult to learn  that distribution directly because it's complex, it's high-dimensional and we  want to be able to get around that complexity. What GANs do is they say,  okay, what if we start from something super, super simple, as simple as it can  get, completely random noise. Could we build a neural network architecture that  can learn to generate synthetic examples from complete random noise? And this is  the underlying concept of GANs, where the goal is to train this generator  network that learns a transformation from noise to the training data  distribution, with the goal of making the generated examples as close to the real  deal as possible. With GANs, the breakthrough idea here was to interface  these two neural networks together, one being a generator and one being a  discriminator, and these two components, the generator and discriminator, are at  war, at competition with each other. Specifically, the goal of the generator  network is to look at random noise and try to produce an imitation of the data  that's as close to real as possible. The discriminator then takes the output  of the generator, as well as some real data examples, and tries to learn a  classification decision distinguishing real from fake, and  effectively in the GAN, these two components are going back and forth,  competing each other, trying to force the discriminator to better learn this  distinction between real and fake, while the generator is trying to fool and  outperform the ability of the discriminator to make that classification.  So, that's the overlying concept, but what I'm really excited about is the next  example, which is one of my absolute favorite illustrations and walkthroughs  in this class, and it gets at the intuition behind GANs, how they work, and  the underlying concept. Okay, we're going to look at a 1D example, points on a line,  right? That's the data that we're working with. In a GAN, the generator  starts from random noise, produces some fake data, they're going to fall  somewhere on this one-dimensional line. Now, the next step is the discriminator  then sees these points, and it also sees some real data. The goal of the  discriminator is to be trained to output a probability that a instance it sees is  real or fake, and initially, in the beginning, before training, it's not  trained, right? So its predictions may not be very good, but over the course of  training, you're going to train it and it hopefully will start increasing the  probability for those examples that are real and decreasing the probability for  those examples that are fake. Overall goal is to predict what is real. Until  eventually the discriminator reaches this point where it has a perfect  separation, perfect classification of real versus fake. Okay, so at this point  the discriminator thinks, okay, I've done my job. Now we go back to the generator  and it sees the examples of where the real data lie, and it can be forced to\n",
            "38.33 - 43.11 min\n",
            " start moving its generated fake data closer and closer, increasingly closer to  the real data. We can then go back to the discriminator, which receives these  newly synthesized examples from the generator and repeats that same process  of estimating the probability that any given point is real, and learning to  increase the probability of the true real examples, decrease the probability  of the fake points, adjusting, adjusting over the course of its training. And  finally we can go back and repeat to the generator again, one last time. The  generator starts moving those fake points closer, closer and closer to the  real data, such that the fake data is almost following the distribution of the  real data. At this point it becomes very, very hard for the discriminator to  distinguish between what is real and what is fake. While the generator will  continue to try to create fake data points to fool the discriminator. This is  really the key concept, the underlying intuition behind how the components of  the GAN are essentially competing with each other, going back and forth between  the generator and the discriminator. And in fact, this is the, this intuitive  concept is how the GAN is trained in practice, where the generator first tries  to synthesize new examples, synthetic examples, to fool the discriminator, and  the goal of the discriminator is to take both the fake examples and the real data  to try to identify the synthesized instances. In training, what this means is  that the objective, the loss for the generator and discriminator, have to be  at odds with each other. They're adversarial, and that is what gives rise  to the component of adversarial in generative adversarial network. These  adversarial objectives are then put together to then define what it means to  arrive at a stable global optimum, where the generator is capable of producing  the true data distribution that would completely fool the discriminator.  Concretely, this can be defined mathematically in terms of a loss  objective, and again, though I'm showing math, we can distill this down and  go through what each of these terms reflect in terms of that core intuitive  idea and conceptual idea that hopefully that 1D example conveyed. So we'll first  consider the perspective of the discriminator D. Its goal is to maximize  probability that its decisions, in its decisions that real data are classified  real, fake data classified as fake. So here, the first term, G of Z is the  generator's output, and D of G of Z is the discriminator's estimate of that  generated output as being fake. D of X, X is the real data, and so D of X is the  estimate of the probability that a real instance is fake. 1 minus D of X is the  estimate that that real instance is real. So here, in both these cases, the  discriminator is producing a decision about fake data, real data, and together  it wants to try to maximize the probability that it's getting answers  correct, right? Now, with the generator, we have those same exact terms, but keep in  mind the generator is never able to affect anything that the discriminator's  decision is actually doing besides generating new data examples. So for the  generator, its objective is simply to minimize the probability that the  generated data is identified as fake.  Together, we want to then put this together to define what it means for the  generator to synthesize fake images that hopefully fool the discriminator. All in  all, right, this term, besides the math, besides the particularities of this  definition, what I want you to get away from this section on GANs is that  we have this dual competing objective where the generator is trying to  synthesize these synthetic examples that ideally fool the best discriminator  possible, and in doing so, the goal is to build up a network via this adversarial\n",
            "43.11 - 48.15 min\n",
            " training, this adversarial competition, to use the generator to create new data  that best mimics the true data distribution and is completely  synthetic new instances. What this amounts to in practice is that after the  training process, you can look exclusively at the generator component  and use it to then create new data instances. All this is done by starting  from random noise and trying to learn a model that goes from random noise to the  real data distribution, and effectively what GANs are doing is learning a  function that transforms that distribution of random noise to some  target. What this mapping does is it allows us to take a particular  observation of noise in that noise space and map it to some output, a particular  output in our target data space, and in turn, if we consider some other random  sample of noise, if we feed it through the generator of the GAN, it's going to  produce a completely new instance falling somewhere else on that true data  distribution manifold. And indeed, what we can actually do is interpolate and  traverse between trajectories in the noise space that then map to traversals  and interpolations in the target data space. And this is really, really cool  because now you can think about an initial point and a target point and all  the steps that are going to take you to synthesize and go between those images  in that target data distribution. So hopefully this gets, gives a sense of  this concept of generative modeling for the purpose of creating new data  instances, and that notion of interpolation and data transformation  leads very nicely to some of the recent advances and applications of GANs, where  one particularly commonly employed idea is to try to iteratively grow the GAN,  to get more and more detailed image generations, progressively adding layers  over the course of training to then refine the examples generated by the  generator. And this is the approach that was used to generate those synthetic,  those images of those synthetic faces that I showed at the beginning of this  lecture. This idea of using a GAN that is refined iteratively to produce higher  resolution images. Another way we can extend this concept is to extend the GAN  architecture to consider particular tasks and impose further structure on  the network itself. One particular idea is to say, okay, what if we have a  particular label or some factor that we want to condition the generation on? We  call this C, and it's supplied to both the generator and the discriminator. What  this will allow us to achieve is paired translation between different types of  data. So for example, we can have images of a street view and we can have images  of the segmentation of that street view, and we can build a GAN that can directly  translate between the street view and the segmentation. Let's make this more  concrete by considering some particular examples. So what I just described was  going from a segmentation label to a street scene. We can also translate  between a satellite view, aerial satellite image, to what is the roadmap  equivalent of that aerial satellite image, or a particular annotation or  labels of the image of a building to the actual visual realization and visual  facade of that building. We can translate between different lighting conditions,  day to night, black and white to color, outlines to a colored photo. All these  cases, and I think in particular the most interesting and impactful to me, is  this translation between street view and aerial view, and this is used to consider,  for example, if you have data from Google Maps, how you can go between a street  view of the map to the aerial image of that. Finally, again, extending the  same concept of translation between one domain to another, another idea is that  of completely unpaired translation, and this uses a particular GAN architecture\n",
            "48.16 - 53.18 min\n",
            " called CycleGAN. So in this video that I'm showing here, the model takes as  input a bunch of images in one domain, and it doesn't necessarily have to have  a corresponding image in another target domain, but it is trained to try to  generate examples in that target domain that roughly correspond to the source  domain, transferring the style of the source onto the target and vice versa. So  this example is showing the translation of images in horse domain to zebra domain.  The concept here is this cyclic dependency, right? You have two GANs that  are connected together via this cyclic loss, transforming between one domain and  another, and really like all the examples that we've seen so far in this lecture,  the intuition is this idea of distribution transformation. Normally  with a GAN, you're going from noise to some target. With the CycleGAN, you're  trying to go from some source distribution, some data manifold X, to a  target distribution, another data manifold Y. And this is really, really  not only cool, but also powerful in thinking about how we can translate  across these different distributions flexibly. And in fact, this is a- allows  us to do transformations not only to images, but to speech and audio as well.  So in the case of speech and audio, turns out that you can take sound waves,  represent it compactly in a spectrogram image, and use a CycleGAN to then  translate and transform speech from one person's voice in one domain to another  person's voice in another domain, right? These are two independent data  distributions that we define. Maybe you're getting a sense of where I'm  hinting at, maybe not, but in fact this was exactly how we developed the model  to synthesize the audio behind Obama's voice that we saw in yesterday's  introductory lecture. What we did was we trained a CycleGAN to take data in  Alexander's voice and transform it into data in the manifold of Obama's voice. So  we can visualize how that spectrogram waveform looks like for Alexander's voice  versus Obama's voice that was completely synthesized using this CycleGAN  approach.  Hi everybody, and welcome to MIT Fit Pass 191, the official introductory course on deep learning taught here at MIT.  Hi everybody.  I replayed it. Okay, but basically what we did was Alexander spoke that exact phrase that was  played yesterday, and we had the trained CycleGAN model, and we can deploy it then  on that exact audio to transform it from the domain of Alexander's voice to  Obama's voice, generating the synthetic audio that was played for that video clip.  Hi everybody.  Alright. Okay, before I accidentally play it again, I jump now to the summary slide.  So today in this lecture we've learned deep generative models, specifically  talking mostly about latent variable models, autoencoders, variational  autoencoders, where our goal is to learn this low-dimensional latent encoding of  the data, as well as generative adversarial networks where we have these  competing generator and discriminator components that are trying to synthesize  synthetic examples. We've talked about these core foundational generative  methods, but it turns out, as I alluded to in the beginning of the lecture, that in  this past year in particular, we've seen truly, truly tremendous advances in  generative modeling, many of which have not been from those two methods, those  two foundational methods that we described, but rather a new approach  called diffusion modeling. Diffusion models are driving, are the driving tools  behind the tremendous advances in generative AI that we've seen in this  past year in particular. VAEs, GANs, they're learning these transformations, these  encodings, but they're largely restricted to generating examples that fall similar\n",
            "53.18 - 57.95 min\n",
            " to the data space that they've seen before. Diffusion models have this  ability to now hallucinate and envision and imagine completely new objects and  instances which we as humans may not have seen or even thought about, right,  parts of the design space that are not covered by the training data. And so an  example here is this AI-generated art, which art if you will, right, which was  created by a diffusion model. And I think not only does this get at some of  the limits and capabilities of these powerful models, but also questions about  what does it mean to create new instances, what are the limits and bounds  of these models, and how do they, how can we think about their advances with  respect to human capabilities and human intelligence. And so I'm really excited  that on Thursday in lecture seven on new frontiers in deep learning, we're  going to take a really deep dive into diffusion models, talk about their  fundamentals, talk about not only applications to images, but other fields  as well in which we're seeing these models really start to make transformative  advances, because they are indeed at the very cutting edge and very much the new  frontier of generative AI today. Alright, so with that tease and hopefully set  the stage for lecture seven on Thursday, and conclude and remind you all that we  have now about an hour for open office hour time for you to work on your  software labs. Come to us, ask any questions you may have, as well as the  TAs who will be here as well. Thank you so much.\n",
            "57.95 - 59.83 min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization using ChromaDB\n",
        "\n",
        "## Introduction to ChromaDB\n",
        "ChromaDB is a high-performance vector database used to transform our text data into a structured, queryable form. In this section, we'll explain what ChromaDB is and why it's useful in our pipeline.\n",
        "\n",
        "## Preprocessing Text for Vectorization\n",
        "Before we can vectorize our text data, it may need to be preprocessed. This section discusses any necessary preprocessing steps such as tokenization or normalization.\n",
        "\n",
        "## Vectorizing Text Data\n",
        "Once our text data is preprocessed, it's time to vectorize it using ChromaDB. We'll explain how to send our text data to ChromaDB, receive vectorized data in return, and store this data for future use."
      ],
      "metadata": {
        "id": "pElv8qIQ3vT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process text and store in VectorDB"
      ],
      "metadata": {
        "id": "xAybb4HaMo-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "embeddings = OpenAIEmbeddings(openai_api_key = os.environ[\"OPENAI_API_KEY\"])\n",
        "#vstore with metadata. Here we will store page numbers.\n",
        "vStore = Chroma.from_texts(documents, embeddings, metadatas=[{\"source\": s} for s in sources])\n",
        "#deciding model\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "retriever = vStore.as_retriever()\n",
        "retriever.search_kwargs = {'k':2}"
      ],
      "metadata": {
        "id": "TZO9mYT_OfcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RetrievalQAWithSourcesChain.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)"
      ],
      "metadata": {
        "id": "W6jspHYeGEbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Querying with Langchain\n",
        "\n",
        "## Introduction to Langchain\n",
        "Langchain provides a natural language interface for querying our vector data. In this section, we'll provide an introduction to Langchain and explain how it fits into our pipeline.\n",
        "\n",
        "## Setting Up Langchain for Querying\n",
        "Before we can start querying, we need to set up Langchain. This section will guide you through the process of setting up Langchain to work with our vectorized data.\n",
        "\n",
        "## Formulating and Executing Queries\n",
        "With Langchain set up, we can now formulate and execute queries on our data. We'll walk you through the process of creating a query, sending it to Langchain, and interpreting the results."
      ],
      "metadata": {
        "id": "fQjQD2Al35Rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q&A"
      ],
      "metadata": {
        "id": "SXDgtwBrkNOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is this video about?\"\n",
        "response = model({\"question\":query}, return_only_outputs=True)\n",
        "print('Answer :',response['answer'])\n",
        "print('Referred clip segments :',response['sources'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14Sap5xtGLZG",
        "outputId": "86497909-a5d6-4d9c-c316-39895d97428c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer :  This video is about deep generative models, specifically latent variable models, autoencoders, variational autoencoders, generative adversarial networks and diffusion models. It also shows an example of CycleGAN which is used to synthesize Obama's voice from Alexander's voice.\n",
            "\n",
            "Referred clip segments : 53.18 - 57.95 min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is a generative ai?\"\n",
        "response = model({\"question\":query}, return_only_outputs=True)\n",
        "print('Answer :',response['answer'])\n",
        "print('Referred clip segments :',response['sources'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQYXQmXAJlTI",
        "outputId": "1498cdb8-c5bf-4cdf-a5d9-b1b3a070540f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer :  Generative AI is a subset of deep learning that is used to generate new data instances based on patterns found in existing data.\n",
            "\n",
            "Referred clip segments : 0.0 - 4.95 min, 57.95 - 59.83 min\n"
          ]
        }
      ]
    }
  ]
}